{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Coordinate Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use what we have learned about marker detection in the last notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again load the camera parameters. This time not only need them for preprocessing our image, but we will also use them when estimating where the markers are relative to the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load camera parameters\n",
    "with open(\"camera_intrinsics.yml\", 'r') as stream:\n",
    "    camera_intrinsics = yaml.safe_load(stream)\n",
    "camera_matrix = np.array(camera_intrinsics[\"camera_matrix\"][\"data\"]).reshape((3,3))\n",
    "distortion_coefficients = np.array(camera_intrinsics[\"distortion_coefficients\"][\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other component for estimating the markers' poses and postions we need to know beforehand, is the size of our markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set marker size\n",
    "marker_size = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect Aruco Markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use the *cv2.aruco.DICT_ARUCO_ORIGINAL* dictionary for detecting the markers in the given image. \n",
    "\n",
    "**Note:** In this specific case, you don't have to undistort the image after loading it, because the function that handles the marker pose estimation (*solvePnP*, which we will use in the next task) will do that internally. So you just have to detect the markers in the undistorted image, image distortions will be accounted for post detection.\n",
    "\n",
    "**Note:** The list that contains the marker ids should be named *ids*, in order for the provided code to run without changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "img = cv2.imread(\"./2024-09-12_17-29-34/img_282.jpg\")\n",
    "\n",
    "### Your code here ###\n",
    "\n",
    "# setup detector\n",
    "\n",
    "\n",
    "# detect markers in the image\n",
    "\n",
    "\n",
    "# draw markers onto the image\n",
    "img_with_markers = None\n",
    "###\n",
    "\n",
    "\n",
    "plt.imshow(img_with_markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use OpenCV's *solvePnP* function to estimate the poses of the markers detected in the previous task. \n",
    "\n",
    "**Note:** The function should get the camera matrix and distortion parameters as arguments, because it internally handles the *undistortion* step that we did separately in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate marker poses using the detected marker corners from the image, the camera parameters, and the (known) size of the marker \n",
    "marker_rvecs = []\n",
    "marker_tvecs = []\n",
    "\n",
    "# marker corners\n",
    "marker_points = np.array([[-marker_size / 2, marker_size / 2, 0],\n",
    "                          [marker_size / 2, marker_size / 2, 0],\n",
    "                          [marker_size / 2, -marker_size / 2, 0],\n",
    "                          [-marker_size / 2, -marker_size / 2, 0]])\n",
    "\n",
    "### Your code here ###\n",
    "\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Use OpenCV's *drawFrameAxes* function to visualise the estimated marker poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the estimated marker poses in the image\n",
    "img_axis = img.copy()\n",
    "### Your code here ###\n",
    "\n",
    "###\n",
    "\n",
    "plt.imshow(img_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Aruco Marker Positions to Robot and World Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix(x_rotation, y_rotation, z_rotation):\n",
    "    alpha = z_rotation\n",
    "    beta = y_rotation\n",
    "    gamma = x_rotation\n",
    "\n",
    "    # https://en.wikipedia.org/wiki/Rotation_matrix\n",
    "    # rotates around z, then y, then x\n",
    "\n",
    "    return np.array([\n",
    "        [np.cos(alpha)*np.cos(beta), np.cos(alpha)*np.sin(beta)*np.sin(gamma)-np.sin(alpha)*np.cos(gamma), np.cos(alpha)*np.sin(beta)*np.cos(gamma)+np.sin(alpha)*np.sin(gamma)],\n",
    "        [np.sin(alpha)*np.cos(beta), np.sin(alpha)*np.sin(beta)*np.sin(gamma)+np.cos(alpha)*np.cos(gamma), np.sin(alpha)*np.sin(beta)*np.cos(gamma)-np.cos(alpha)*np.sin(gamma)],\n",
    "        [-np.sin(beta), np.cos(beta)*np.sin(gamma), np.cos(beta)*np.cos(gamma)]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectors_to_transformation_matrix(rotation, translation):\n",
    "    ### Your code here ###\n",
    "\n",
    "    ###\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation_matrix_to_vectors(t):\n",
    "    ### Your code here ###\n",
    "\n",
    "    ###\n",
    "    return np.array([x_rotation, y_rotation, z_rotation]), translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting to robot frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset from camera to robot frame\n",
    "x_offset_camera = 0.03\n",
    "y_offset_camera = 0\n",
    "z_offset_camera = 0.28\n",
    "\n",
    "offset_camera_robot = np.array([x_offset_camera,y_offset_camera,z_offset_camera]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# angle from camera to robot frame\n",
    "x_angle_camera = -140 #degrees\n",
    "y_angle_camera = 0\n",
    "z_angle_camera = -90 #degrees\n",
    "\n",
    "rotation_camera_robot = np.array([np.radians(x_angle_camera), np.radians(y_angle_camera), np.radians(z_angle_camera)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Create transform matrix from camera frame to robot frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n",
    "T_camera_robot = None\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Create transform matrices for all markers, project them into the robot coordinate frame, and transform the resulting matrices back into rotation and translation vectors. Also print the id and world position for each marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transform matrices for all markers, project them into the robot coordinate frame\n",
    "# transform them to rotation and translation vectors\n",
    "marker_rvecs_robot = []\n",
    "marker_tvecs_robot = []\n",
    "### Your code here ###\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us visualise the results of our pose estimation. We start with a visualisation of where the detected markers are, realitve to our robot, on the vertical plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(marker_tvecs_robot):\n",
    "    id = ids[i]\n",
    "    plt.scatter([c[1]], [c[2]], color=\"black\", marker=\"s\")\n",
    "    plt.text(c[1], c[2], str(id))\n",
    "    plt.ylabel(\"z\")\n",
    "    plt.xlabel(\"y\")\n",
    "    plt.xlim([-0.3, 0.3])\n",
    "    plt.ylim([-0.5, 0.5])\n",
    "    plt.axhline(0,-1,1, color=\"lightgray\", ls=\"--\")\n",
    "    plt.text(0.25, 0.01, \"floor\", color=\"lightgray\")\n",
    "\n",
    "plt.gca().invert_xaxis()\n",
    "plt.title(\"vertical plane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, although all markers in the image are obviously placed directly on the floor, our estimated positions don't line uo perfectly with it. Some markers are estimated to \"float\" a bit above the floor, some are estimated to be slightly below it. This kind of imperfect measurement is something we must always expect when working with imperfect data. In our example, our camera calibration might not be completely optimal our images might be noisy, etc. Overall, however, these results aren't too bad and definitely still usable for our purposes. \n",
    "\n",
    "Next, we visualise where the markers are on the horizontal plane (i.e. the floor), relative to our robot. Because the robot is the reference point, it will always be at position (0,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(marker_tvecs_robot):\n",
    "    id = ids[i]\n",
    "    plt.scatter([c[1]], [c[0]], color=\"black\", marker=\"s\")\n",
    "    plt.text(c[1], c[0], str(id))\n",
    "    plt.ylabel(\"x [m]\")\n",
    "    plt.xlabel(\"y [m]\")\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.scatter([0], [0], color=\"red\", marker=\"s\")\n",
    "plt.text(0, 0, \"Robot\")\n",
    "plt.gca().invert_xaxis()\n",
    "plt.title(\"horizontal plane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting to world frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know, where the markers are relative to our robot, we want to determine, where they are in our world coordinate frame. We will later need this to, for example, implement the landmark detection and localisation, as well as the SLAM algorithm for our robot.  \n",
    "Let's assume that our robot started at the origin of the world coordinate frame. Now it has moved 40cm forwards and 25cm to the left, with a current heading o 30Â° to the right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# offset from robot to world frame\n",
    "x_offset_world = 0.4\n",
    "y_offset_world = 0.25\n",
    "z_offset_world = 0\n",
    "\n",
    "# angle from robot to world frame\n",
    "x_angle_world = 0 #degrees\n",
    "y_angle_world = 0\n",
    "z_angle_world = -30\n",
    "\n",
    "offset_robot_world = np.array([x_offset_world,y_offset_world,z_offset_world]) \n",
    "rotation_robot_world = np.array([np.radians(x_angle_world), np.radians(y_angle_world), np.radians(z_angle_world)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Create transform matrix from robot to world frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###\n",
    "T_robot_world = None\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Create transform matrices for all markers in the robot coordinate frame, project them into the world coordinate frame, and transform them to rotation and translation vectors. Also print the id and world position for each marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_rvecs_world = []\n",
    "marker_tvecs_world = []\n",
    "### Your code here ###\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now again visualise our results. First, we plot where our markers are relative to the floor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(marker_tvecs_world):\n",
    "    id = ids[i]\n",
    "    # we mirror the y coordinate, because in the robot coordinate system y < 0 indicates a position to the right of the \n",
    "    # robot, but in the pyplot visualisation values < 0 are displayed on the left. The x ticks are adjusted accordingly.\n",
    "    plt.scatter([c[1]], [c[2]], color=\"black\", marker=\"s\")\n",
    "    plt.text(c[1], c[2], str(id))\n",
    "    plt.ylabel(\"z [m]\")\n",
    "    plt.xlabel(\"y [m]\")\n",
    "    plt.xlim([0.2, 0.8])\n",
    "    plt.ylim([-0.5, 0.5])\n",
    "    plt.axhline(0,-1,1, color=\"lightgray\", ls=\"--\")\n",
    "    plt.text(0.22, 0.01, \"floor\", color=\"lightgray\")\n",
    "    plt.xticks([0.3,0.4,0.5,0.6,0.7], [-0.3,-0.4,-0.5,-0.6,-0.7])\n",
    "plt.title(\"vertical plane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurement error from the previous pose estimation step obviously still exists, but overall our measurement is still good enough to use it, for example for navigation.\n",
    "\n",
    "Finally, we again visualise the positions of the markers in the horizontal plane. This time we also visualise the position of our robot. This gives us essentially a section of our world map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, c in enumerate(marker_tvecs_world):\n",
    "    id = ids[i]\n",
    "    plt.scatter([c[1]], [c[0]], color=\"black\", marker=\"s\")\n",
    "    plt.text(c[1], c[0], str(id))\n",
    "    plt.ylabel(\"x [m]\")\n",
    "    plt.xlabel(\"y [m]\")\n",
    "plt.xlim(-1, 1.5)\n",
    "plt.ylim(-1, 1.5)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.scatter([y_offset_world], [x_offset_world], color=\"red\", marker=\"s\")\n",
    "plt.text(y_offset_world, x_offset_world, \"Robot\")\n",
    "plt.title(\"horizontal plane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
